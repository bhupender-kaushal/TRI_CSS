{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn3x3 = TripletAttention(kernel_size=3)\n",
    "attn5x5 = TripletAttention(kernel_size=5)\n",
    "norm_layer=nn.InstanceNorm2d\n",
    "class SCTA_main(nn.Module):\n",
    "    def __init__(self, inplanes, planes, groups=32, ratio=16):\n",
    "        super().__init__()\n",
    "        d = max(planes // ratio, 32)\n",
    "        self.planes = planes\n",
    "        self.split_3x3 = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, planes, kernel_size=3, padding=1,stride=2, groups=groups),\n",
    "            norm_layer(planes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.split_5x5 = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, planes, kernel_size=5, padding=2,stride=2, groups=groups),\n",
    "            norm_layer(planes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(planes, d),\n",
    "            nn.InstanceNorm1d(d),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(d, planes)\n",
    "        self.fc2 = nn.Linear(d, planes)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        print(x.shape)\n",
    "        u1 = attn3x3(self.split_3x3(x))\n",
    "        u2 = attn5x5(self.split_5x5(x))\n",
    "        u = u1 + u2\n",
    "        s = self.avgpool(u).flatten(1)\n",
    "        z = self.fc(s)\n",
    "        attn_scores = torch.cat([self.fc1(z), self.fc2(z)], dim=1)\n",
    "        attn_scores = attn_scores.view(batch_size, 2, self.planes)\n",
    "        attn_scores = attn_scores.softmax(dim=1)\n",
    "        a = attn_scores[:,0].view(batch_size, self.planes, 1, 1)\n",
    "        b = attn_scores[:,1].view(batch_size, self.planes, 1, 1)\n",
    "        u1 = u1 * a.expand_as(u1)\n",
    "        u2 = u2 * b.expand_as(u2)\n",
    "        x = u1 + u2\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ks):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=ks, stride=1,\n",
    "                              padding=(ks - 1) // 2)\n",
    "        self.bn = norm_layer(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ZPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x_mean = x.mean(dim=1, keepdim=True)\n",
    "        x_max = x.max(dim=1, keepdim=True)[0]\n",
    "        return torch.cat([x_mean, x_max], dim=1)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.compress = ZPool()\n",
    "        self.conv = BasicConv2d(2, 1, kernel_size)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.compress(x)\n",
    "        y = self.conv(y)\n",
    "        y = self.activation(y)\n",
    "        return x * y\n",
    "    \n",
    "class TripletAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.ch = AttentionGate(kernel_size)\n",
    "        self.cw = AttentionGate(kernel_size)\n",
    "        self.hw = AttentionGate(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x_ch = self.ch(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1) # c and h\n",
    "        x_cw = self.cw(x.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
    "        x_hw = self.hw(x)\n",
    "        return 1 / 3 * (x_ch + x_cw + x_hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 32, 32])\n",
      "torch.Size([1, 128, 16, 16])\n",
      "torch.Size([1, 128, 16, 16])\n",
      "torch.Size([1, 64, 32, 32])\n",
      "torch.Size([1, 128, 16, 16])\n",
      "torch.Size([1, 128, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "nf = 64\n",
    "input_nc=2\n",
    "norm_layer = nn.InstanceNorm2d\n",
    "model = [nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_nc, nf, kernel_size=7, padding=0),\n",
    "            norm_layer(nf),\n",
    "            nn.ReLU(True)]\n",
    "model=nn.Sequential(*model)\n",
    "x = torch.randn(1, 2, 32, 32)\n",
    "x=model(x)\n",
    "n_downsampling = 2\n",
    "for i in range(n_downsampling):  # add downsampling layers\n",
    "    mult = 2 ** i\n",
    "    attn = SCTA_main( nf, nf*2)\n",
    "    y = attn(x)\n",
    "    print(y.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
